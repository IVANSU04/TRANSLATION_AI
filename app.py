import streamlit as st
from translator_core_new import generate_translation_and_advice
import streamlit.components.v1 as components
import json
import speech_recognition as sr
import os

# UI Translations
TRANSLATIONS = {
    "zh": {
        "title": "è·¨æ–‡åŒ–æ™ºèƒ½ç¿»è¯‘åŠ©æ‰‹ï¼ˆMVPï¼‰",
        "version_info": "å½“å‰ç‰ˆæœ¬ï¼šæ”¯æŒæ–‡æœ¬è¾“å…¥ã€éº¦å…‹é£å½•éŸ³ä¸æµè§ˆå™¨è¯­éŸ³è¾“å…¥ã€‚",
        "input_label": "è¯·è¾“å…¥è¦ç¿»è¯‘çš„å†…å®¹ï¼š",
        "source_lang": "æºè¯­è¨€",
        "target_lang": "ç›®æ ‡è¯­è¨€",
        "lang_zh": "ä¸­æ–‡",
        "lang_en": "è‹±æ–‡",
        "lang_ja": "æ—¥æ–‡",
        "scenario_label": "ä½¿ç”¨åœºæ™¯",
        "scenario_tourism": "æ—…æ¸¸ / é—®è·¯ / ç”Ÿæ´»åœºæ™¯",
        "scenario_dining": "é¤æ¡ŒèŠå¤© / é¥®é£Ÿåœºæ™¯",
        "scenario_casual": "æ—¥å¸¸é—²èŠ",
        "scenario_business": "å•†åŠ¡ / åŠæ­£å¼åœºåˆ",
        "tone_label": "è¯­æ°”åå¥½",
        "tone_casual": "éšå’Œ",
        "tone_neutral": "ä¸­æ€§",
        "tone_polite": "æ­£å¼/ç¤¼è²Œ",
        "translate_btn": "ç¿»è¯‘å¹¶ç»™å‡ºæ–‡åŒ–å»ºè®®",
        "input_warning": "è¯·è¾“å…¥è¦ç¿»è¯‘çš„å†…å®¹ã€‚",
        "spinner": "æ­£åœ¨ç”Ÿæˆç¿»è¯‘å’Œæ–‡åŒ–å»ºè®®...",
        "literal_title": "ç›´è¯‘",
        "tts_literal_btn": "ğŸ”Š æœ—è¯»ç›´è¯‘",
        "natural_title": "æ›´è‡ªç„¶çš„è¡¨è¾¾",
        "advice_title": "æ–‡åŒ–å»ºè®®",
        "voice_input_browser": "ğŸ¤ æµè§ˆå™¨è¯­éŸ³",
        "voice_input_mic": "ğŸ™ï¸ éº¦å…‹é£å½•éŸ³",
    },
    "en": {
        "title": "Cross-Cultural Translation Assistant (MVP)",
        "version_info": "Current version: Supports text input, microphone recording, and browser voice input.",
        "input_label": "Enter text to translate:",
        "source_lang": "Source Language",
        "target_lang": "Target Language",
        "lang_zh": "Chinese",
        "lang_en": "English",
        "lang_ja": "Japanese",
        "scenario_label": "Scenario",
        "scenario_tourism": "Tourism / Directions / Daily Life",
        "scenario_dining": "Dining / Food",
        "scenario_casual": "Casual Chat",
        "scenario_business": "Business / Semi-formal",
        "tone_label": "Tone Preference",
        "tone_casual": "Casual",
        "tone_neutral": "Neutral",
        "tone_polite": "Polite/Formal",
        "translate_btn": "Translate & Get Cultural Advice",
        "input_warning": "Please enter text to translate.",
        "spinner": "Generating translation and advice...",
        "literal_title": "Literal Translation",
        "tts_literal_btn": "ğŸ”Š Read Literal",
        "natural_title": "Natural Expressions",
        "advice_title": "Cultural Advice",
        "voice_input_browser": "ğŸ¤ Browser Voice",
        "voice_input_mic": "ğŸ™ï¸ Mic Recording",
    },
    "ja": {
        "title": "ç•°æ–‡åŒ–ç¿»è¨³ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ (MVP)",
        "version_info": "ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼šãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã€ãƒã‚¤ã‚¯éŒ²éŸ³ã€ãƒ–ãƒ©ã‚¦ã‚¶éŸ³å£°å…¥åŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã€‚",
        "input_label": "ç¿»è¨³ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š",
        "source_lang": "ç¿»è¨³å…ƒã®è¨€èª",
        "target_lang": "ç¿»è¨³å…ˆã®è¨€èª",
        "lang_zh": "ä¸­å›½èª",
        "lang_en": "è‹±èª",
        "lang_ja": "æ—¥æœ¬èª",
        "scenario_label": "åˆ©ç”¨ã‚·ãƒ¼ãƒ³",
        "scenario_tourism": "è¦³å…‰ / é“æ¡ˆå†… / ç”Ÿæ´»",
        "scenario_dining": "é£Ÿäº‹ / ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³",
        "scenario_casual": "æ—¥å¸¸ä¼šè©±",
        "scenario_business": "ãƒ“ã‚¸ãƒã‚¹ / ã‚»ãƒŸãƒ•ã‚©ãƒ¼ãƒãƒ«",
        "tone_label": "å£èª¿",
        "tone_casual": "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«",
        "tone_neutral": "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«",
        "tone_polite": "ä¸å¯§ / ãƒ•ã‚©ãƒ¼ãƒãƒ«",
        "translate_btn": "ç¿»è¨³ã—ã¦æ–‡åŒ–çš„ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’è¡¨ç¤º",
        "input_warning": "ç¿»è¨³ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "spinner": "ç¿»è¨³ã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç”Ÿæˆä¸­...",
        "literal_title": "ç›´è¨³",
        "tts_literal_btn": "ğŸ”Š ç›´è¨³ã‚’èª­ã¿ä¸Šã’",
        "natural_title": "ã‚ˆã‚Šè‡ªç„¶ãªè¡¨ç¾",
        "advice_title": "æ–‡åŒ–çš„ã‚¢ãƒ‰ãƒã‚¤ã‚¹",
        "voice_input_browser": "ğŸ¤ ãƒ–ãƒ©ã‚¦ã‚¶éŸ³å£°",
        "voice_input_mic": "ğŸ™ï¸ ãƒã‚¤ã‚¯éŒ²éŸ³",
    }
}


def play_text_js(text, lang):
    """
    Generate and execute JavaScript to play audio using the browser's built-in SpeechSynthesis API.
    This avoids backend network issues (403 Forbidden, connection errors) by running entirely in the client's browser.
    """
    lang_map = {
        "zh": "zh-CN",
        "en": "en-US",
        "ja": "ja-JP"
    }
    js_lang = lang_map.get(lang, "en-US")
    safe_text = json.dumps(text)
    
    js_code = f"""
    <script>
        try {{
            window.parent.speechSynthesis.cancel();
            var msg = new SpeechSynthesisUtterance({safe_text});
            msg.lang = "{js_lang}";
            msg.rate = 1.0; 
            window.parent.speechSynthesis.speak(msg);
        }} catch (e) {{
            console.error("TTS Error:", e);
        }}
    </script>
    """
    components.html(js_code, height=0)


def browser_speech_recognition_js(lang_code):
    """
    Use browser's native Web Speech API for voice input.
    This is a client-side solution that works on all Python versions.
    """
    lang_map = {"zh": "zh-CN", "en": "en-US", "ja": "ja-JP"}
    recognition_lang = lang_map.get(lang_code, "en-US")
    
    # Use a unique ID to store results
    unique_id = f"speech_result_{hash(recognition_lang) % 10000}"
    
    js_code = f"""
    <script>
        (function() {{
            try {{
                const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
                recognition.lang = "{recognition_lang}";
                recognition.continuous = false;
                recognition.interimResults = false;
                
                recognition.onstart = function() {{
                    console.log('Speech recognition started');
                }};
                
                recognition.onresult = function(event) {{
                    const transcript = event.results[0][0].transcript;
                    console.log('Speech result:', transcript);
                    
                    // Store result in session storage for Streamlit to read
                    sessionStorage.setItem('{unique_id}', transcript);
                    
                    // Try to trigger a Streamlit rerun by dispatching an event
                    window.parent.postMessage({{
                        type: 'streamlit:setComponentValue',
                        value: transcript
                    }}, '*');
                }};
                
                recognition.onerror = function(event) {{
                    console.error('Speech recognition error:', event.error);
                    sessionStorage.setItem('{unique_id}_error', event.error);
                }};
                
                recognition.onend = function() {{
                    console.log('Speech recognition ended');
                }};
                
                recognition.start();
            }} catch (e) {{
                console.error('Browser speech recognition not supported:', e);
                alert('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½ã€‚è¯·ä½¿ç”¨ Chrome/Edge æµè§ˆå™¨ï¼Œæˆ–ä½¿ç”¨éº¦å…‹é£å½•éŸ³åŠŸèƒ½ã€‚');
            }}
        }})();
    </script>
    <div style="text-align: center; padding: 10px; background: #e3f2fd; border-radius: 5px; margin: 10px 0;">
        <p style="margin: 0; color: #1976d2;">ğŸ¤ æ­£åœ¨ç›‘å¬... è¯·å¼€å§‹è¯´è¯</p>
        <p style="margin: 5px 0 0 0; font-size: 12px; color: #666;">
            è¯´å®Œåè¯·ç­‰å¾…å‡ ç§’ï¼Œç„¶åæ‰‹åŠ¨ç‚¹å‡»ä¸‹æ–¹æ–‡æœ¬æ¡†æŸ¥çœ‹è¯†åˆ«ç»“æœ
        </p>
    </div>
    """
    components.html(js_code, height=120)
    
    st.info("ğŸ’¡ æç¤ºï¼šæµè§ˆå™¨è¯­éŸ³è¯†åˆ«ç»“æœä¼šæ˜¾ç¤ºåœ¨æµè§ˆå™¨æ§åˆ¶å°ã€‚ç”±äºæŠ€æœ¯é™åˆ¶ï¼Œè¯·è¯´å®Œåæ‰‹åŠ¨åˆ·æ–°é¡µé¢æˆ–ç‚¹å‡»æ–‡æœ¬æ¡†æŸ¥çœ‹ç»“æœã€‚æ¨èä½¿ç”¨éº¦å…‹é£å½•éŸ³åŠŸèƒ½è·å¾—æ›´å¥½çš„ä½“éªŒã€‚")


def recognize_speech_from_mic(lang_code):
    """
    Capture audio from the microphone and transcribe it.
    Prioritizes Google Speech Recognition (online).
    Falls back to Vosk (offline) if Google fails and Vosk model is available.
    """
    r = sr.Recognizer()
    
    # Map app language codes to Google Speech Recognition codes
    google_lang_map = {
        "zh": "zh-CN",
        "en": "en-US",
        "ja": "ja-JP"
    }
    target_lang = google_lang_map.get(lang_code, "en-US")

    # Map app language codes to Vosk model paths (relative to app.py)
    vosk_model_map = {
        "zh": "models/zh",
        "en": "models/en",
        "ja": "models/ja"
    }
    vosk_model_path = vosk_model_map.get(lang_code)

    try:
        with sr.Microphone() as source:
            st.info(f"ğŸ™ï¸ æ­£åœ¨ç›‘å¬ ({target_lang})...")
            r.adjust_for_ambient_noise(source, duration=0.5)
            audio = r.listen(source, timeout=5, phrase_time_limit=15)
            
            st.info("ğŸ”„ æ­£åœ¨è¯†åˆ«...")
            
            # 1. Try Google (Online)
            try:
                text = r.recognize_google(audio, language=target_lang)
                st.success("âœ… è¯†åˆ«æˆåŠŸï¼")
                return text
            except sr.RequestError as e:
                # Network error (e.g. GFW blocking Google)
                st.warning(f"âš ï¸ Google è¯­éŸ³æœåŠ¡è¿æ¥å¤±è´¥: {e}")
                
                # 2. Try Vosk (Offline) as fallback
                if vosk_model_path and os.path.exists(vosk_model_path):
                    st.info(f"ğŸ”„ å°è¯• Vosk ç¦»çº¿è¯†åˆ« (æ¨¡å‹: {vosk_model_path})...")
                    try:
                        from vosk import Model, KaldiRecognizer
                        
                        model = Model(vosk_model_path)
                        rec = KaldiRecognizer(model, 16000)
                        
                        # Convert audio data to bytes
                        data = audio.get_raw_data(convert_rate=16000, convert_width=2)
                        if rec.AcceptWaveform(data):
                            res = json.loads(rec.Result())
                            return res.get("text", "")
                        else:
                            res = json.loads(rec.FinalResult())
                            return res.get("text", "")
                            
                    except ImportError:
                        st.error("âŒ Vosk åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install vosk")
                    except Exception as vosk_e:
                        st.error(f"âŒ Vosk è¯†åˆ«å¤±è´¥: {vosk_e}")
                else:
                    st.error("âŒ ç¦»çº¿è¯†åˆ«ä¸å¯ç”¨ã€‚è¯·ä¸‹è½½ Vosk æ¨¡å‹å¹¶è§£å‹åˆ° `models/zh` (æˆ– en/ja) æ–‡ä»¶å¤¹ã€‚")
                    with st.expander("ğŸ“– å¦‚ä½•å¯ç”¨ç¦»çº¿è¯­éŸ³è¯†åˆ« (Vosk)"):
                        st.markdown("""
                        **æ­¥éª¤ï¼š**
                        1. ä¸‹è½½å¯¹åº”è¯­è¨€çš„æ¨¡å‹ (https://alphacephei.com/vosk/models)
                           - ä¸­æ–‡: `vosk-model-small-cn-0.22`
                           - è‹±æ–‡: `vosk-model-small-en-us-0.15`
                           - æ—¥æ–‡: `vosk-model-small-ja-0.22`
                        2. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `models` æ–‡ä»¶å¤¹
                        3. è§£å‹ä¸‹è½½çš„æ¨¡å‹ï¼Œé‡å‘½åä¸º `zh`ã€`en` æˆ– `ja`ï¼Œæ”¾å…¥ `models` æ–‡ä»¶å¤¹
                        4. å®‰è£… vosk: `pip install vosk`
                        """)
                return None
                
    except sr.WaitTimeoutError:
        st.warning("â±ï¸ æœªæ£€æµ‹åˆ°è¯­éŸ³")
    except sr.UnknownValueError:
        st.warning("â“ æ— æ³•ç†è§£éŸ³é¢‘å†…å®¹")
    except Exception as e:
        st.error(f"âŒ éº¦å…‹é£æˆ–ç³»ç»Ÿé”™è¯¯: {e}")
    return None


def main():
    # Sidebar for language selection
    with st.sidebar:
        st.header("Settings / è®¾ç½® / è¨­å®š")
        ui_lang_option = st.selectbox(
            "ç•Œé¢è¯­è¨€ / Interface Language",
            ["ä¸­æ–‡", "English", "æ—¥æœ¬èª"],
            index=0
        )
        
        # Show Python version
        import sys
        python_version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
        st.caption(f"ğŸ Python {python_version}")
        st.caption("âœ… éº¦å…‹é£å½•éŸ³ + æµè§ˆå™¨è¯­éŸ³è¾“å…¥")
    
    lang_code_map = {"ä¸­æ–‡": "zh", "English": "en", "æ—¥æœ¬èª": "ja"}
    ui_lang = lang_code_map[ui_lang_option]
    t = TRANSLATIONS[ui_lang]

    st.title(t["title"])
    st.write(t["version_info"])

    # Initialize session state for input text
    if "input_text" not in st.session_state:
        st.session_state.input_text = ""

    # 1. Language Selection (Moved to top for Voice Input context)
    col1, col2 = st.columns(2)
    with col1:
        source_lang = st.selectbox(t["source_lang"], ["zh", "en", "ja"], index=0,
                                   format_func=lambda x: {"zh": t["lang_zh"], "en": t["lang_en"], "ja": t["lang_ja"]}[x])
    with col2:
        target_lang = st.selectbox(t["target_lang"], ["zh", "en", "ja"], index=1,
                                   format_func=lambda x: {"zh": t["lang_zh"], "en": t["lang_en"], "ja": t["lang_ja"]}[x])

    # 2. Input Area with Voice Input Options
    st.subheader(t["input_label"])
    
    # Voice input buttons in columns
    voice_col1, voice_col2, voice_col3 = st.columns([1, 1, 3])
    
    with voice_col1:
        # Microphone Recording (traditional method)
        if st.button(t["voice_input_mic"], key="mic_recording", use_container_width=True):
            recognized_text = recognize_speech_from_mic(source_lang)
            if recognized_text:
                st.session_state.input_text = recognized_text
                st.rerun()
    
    with voice_col2:
        # Browser Voice Input (Web Speech API)
        if st.button(t["voice_input_browser"], key="browser_voice", use_container_width=True):
            browser_speech_recognition_js(source_lang)
    
    with voice_col3:
        st.caption("ğŸ™ï¸ æ¨èä½¿ç”¨éº¦å…‹é£å½•éŸ³ | ğŸ¤ æµè§ˆå™¨è¯­éŸ³ä¸ºå¤‡é€‰æ–¹æ¡ˆ")

    # Text input area
    def update_input():
        st.session_state.input_text = st.session_state.widget_input

    source_text = st.text_area(
        label="Input Text",
        value=st.session_state.input_text,
        height=150,
        key="widget_input",
        on_change=update_input,
        label_visibility="collapsed",
        placeholder="åœ¨æ­¤è¾“å…¥æˆ–ç²˜è´´æ–‡æœ¬ï¼Œæˆ–ä½¿ç”¨ä¸Šæ–¹çš„è¯­éŸ³è¾“å…¥æŒ‰é’®..."
    )

    # 3. Other Parameters
    param_col1, param_col2 = st.columns(2)
    
    with param_col1:
        scenario = st.selectbox(
            t["scenario_label"],
            ["tourism", "dining", "casual_chat", "business"],
            index=0,
            format_func=lambda x: {
                "tourism": t["scenario_tourism"],
                "dining": t["scenario_dining"],
                "casual_chat": t["scenario_casual"],
                "business": t["scenario_business"],
            }[x],
        )
    
    with param_col2:
        tone = st.selectbox(
            t["tone_label"],
            ["casual", "neutral", "polite"],
            index=1,
            format_func=lambda x: {
                "casual": t["tone_casual"],
                "neutral": t["tone_neutral"],
                "polite": t["tone_polite"],
            }[x],
        )

    # 4. Translate Button
    if "translation_result" not in st.session_state:
        st.session_state.translation_result = None

    if st.button(t["translate_btn"], type="primary", use_container_width=True):
        if not source_text or not source_text.strip():
            st.warning(t["input_warning"])
        else:
            with st.spinner(t["spinner"]):
                result = generate_translation_and_advice(
                    source_text=source_text,
                    source_lang=source_lang,
                    target_lang=target_lang,
                    scenario=scenario,
                    tone=tone,
                )
                st.session_state.translation_result = result

    # 5. Results Display
    if st.session_state.translation_result:
        result = st.session_state.translation_result
        
        st.divider()
        
        # Literal Translation
        st.subheader(t["literal_title"])
        literal_text = result.get("literal_translation", "")
        st.write(literal_text)
        
        # TTS button for literal translation
        if literal_text and not literal_text.startswith("["):
            clean_literal = literal_text.replace("ç›´è¯‘ï¼š", "").strip()
            if st.button(t["tts_literal_btn"], key="tts_literal"):
                play_text_js(clean_literal, target_lang)

        st.divider()
        
        # Natural Expressions
        st.subheader(t["natural_title"])
        natural_data = result.get("natural_translation", [])
        
        if isinstance(natural_data, list):
            for idx, item in enumerate(natural_data):
                text = item.get("text", "")
                explanation = item.get("explanation", "")
                
                col_text, col_btn = st.columns([5, 1])
                with col_text:
                    st.markdown(f"**{idx + 1}. {text}**")
                    if explanation:
                        st.caption(explanation)
                with col_btn:
                    if text and not text.startswith("["):
                        if st.button("ğŸ”Š", key=f"tts_natural_{idx}"):
                            play_text_js(text, target_lang)
        else:
            st.write(natural_data)

        st.divider()
        
        # Cultural Advice
        st.subheader(t["advice_title"])
        st.markdown(result.get("advice", ""))


if __name__ == "__main__":
    main()
