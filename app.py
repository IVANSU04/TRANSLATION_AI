import streamlit as st
from translator_core_new import generate_translation_and_advice
import streamlit.components.v1 as components
import json
import speech_recognition as sr  # æ¢å¤ä½¿ç”¨
import os

# UI Translations
TRANSLATIONS = {
    "zh": {
        "title": "è·¨æ–‡åŒ–æ™ºèƒ½ç¿»è¯‘åŠ©æ‰‹ï¼ˆMVPï¼‰",
        "version_info": "å½“å‰ç‰ˆæœ¬ï¼šæ”¯æŒæ–‡æœ¬è¾“å…¥ã€éº¦å…‹é£å½•éŸ³ä¸æµè§ˆå™¨è¯­éŸ³è¾“å…¥ã€‚",
        "input_label": "è¯·è¾“å…¥è¦ç¿»è¯‘çš„å†…å®¹ï¼š",
        "source_lang": "æºè¯­è¨€",
        "target_lang": "ç›®æ ‡è¯­è¨€",
        "lang_zh": "ä¸­æ–‡",
        "lang_en": "è‹±æ–‡",
        "lang_ja": "æ—¥æ–‡",
        "scenario_label": "ä½¿ç”¨åœºæ™¯",
        "scenario_tourism": "æ—…æ¸¸ / é—®è·¯ / ç”Ÿæ´»åœºæ™¯",
        "scenario_dining": "é¤æ¡ŒèŠå¤© / é¥®é£Ÿåœºæ™¯",
        "scenario_casual": "æ—¥å¸¸é—²èŠ",
        "scenario_business": "å•†åŠ¡ / åŠæ­£å¼åœºåˆ",
        "tone_label": "è¯­æ°”åå¥½",
        "tone_casual": "éšå’Œ",
        "tone_neutral": "ä¸­æ€§",
        "tone_polite": "æ­£å¼/ç¤¼è²Œ",
        "translate_btn": "ç¿»è¯‘å¹¶ç»™å‡ºæ–‡åŒ–å»ºè®®",
        "input_warning": "è¯·è¾“å…¥è¦ç¿»è¯‘çš„å†…å®¹ã€‚",
        "spinner": "æ­£åœ¨ç”Ÿæˆç¿»è¯‘å’Œæ–‡åŒ–å»ºè®®...",
        "literal_title": "ç›´è¯‘",
        "tts_literal_btn": "ğŸ”Š æœ—è¯»ç›´è¯‘",
        "natural_title": "æ›´è‡ªç„¶çš„è¡¨è¾¾",
        "advice_title": "æ–‡åŒ–å»ºè®®",
        "voice_input_browser": "ğŸ¤ æµè§ˆå™¨è¯­éŸ³",
        # "voice_input_mic": "ğŸ™ï¸ éº¦å…‹é£å½•éŸ³",  # æš‚æ—¶æ³¨é‡Š
    },
    "en": {
        "title": "Cross-Cultural Translation Assistant (MVP)",
        "version_info": "Current version: Supports text input, microphone recording, and browser voice input.",
        "input_label": "Enter text to translate:",
        "source_lang": "Source Language",
        "target_lang": "Target Language",
        "lang_zh": "Chinese",
        "lang_en": "English",
        "lang_ja": "Japanese",
        "scenario_label": "Scenario",
        "scenario_tourism": "Tourism / Directions / Daily Life",
        "scenario_dining": "Dining / Food",
        "scenario_casual": "Casual Chat",
        "scenario_business": "Business / Semi-formal",
        "tone_label": "Tone Preference",
        "tone_casual": "Casual",
        "tone_neutral": "Neutral",
        "tone_polite": "Polite/Formal",
        "translate_btn": "Translate & Get Cultural Advice",
        "input_warning": "Please enter text to translate.",
        "spinner": "Generating translation and advice...",
        "literal_title": "Literal Translation",
        "tts_literal_btn": "ğŸ”Š Read Literal",
        "natural_title": "Natural Expressions",
        "advice_title": "Cultural Advice",
        "voice_input_browser": "ğŸ¤ Browser Voice",
        # "voice_input_mic": "ğŸ™ï¸ Mic Recording",  # æš‚æ—¶æ³¨é‡Š
    },
    "ja": {
        "title": "ç•°æ–‡åŒ–ç¿»è¨³ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ (MVP)",
        "version_info": "ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼šãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã€ãƒã‚¤ã‚¯éŒ²éŸ³ã€ãƒ–ãƒ©ã‚¦ã‚¶éŸ³å£°å…¥åŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã€‚",
        "input_label": "ç¿»è¨³ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š",
        "source_lang": "ç¿»è¨³å…ƒã®è¨€èª",
        "target_lang": "ç¿»è¨³å…ˆã®è¨€èª",
        "lang_zh": "ä¸­å›½èª",
        "lang_en": "è‹±èª",
        "lang_ja": "æ—¥æœ¬èª",
        "scenario_label": "åˆ©ç”¨ã‚·ãƒ¼ãƒ³",
        "scenario_tourism": "è¦³å…‰ / é“æ¡ˆå†… / ç”Ÿæ´»",
        "scenario_dining": "é£Ÿäº‹ / ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³",
        "scenario_casual": "æ—¥å¸¸ä¼šè©±",
        "scenario_business": "ãƒ“ã‚¸ãƒã‚¹ / ã‚»ãƒŸãƒ•ã‚©ãƒ¼ãƒãƒ«",
        "tone_label": "å£èª¿",
        "tone_casual": "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«",
        "tone_neutral": "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«",
        "tone_polite": "ä¸å¯§ / ãƒ•ã‚©ãƒ¼ãƒãƒ«",
        "translate_btn": "ç¿»è¨³ã—ã¦æ–‡åŒ–çš„ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’è¡¨ç¤º",
        "input_warning": "ç¿»è¨³ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "spinner": "ç¿»è¨³ã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç”Ÿæˆä¸­...",
        "literal_title": "ç›´è¨³",
        "tts_literal_btn": "ğŸ”Š ç›´è¨³ã‚’èª­ã¿ä¸Šã’",
        "natural_title": "ã‚ˆã‚Šè‡ªç„¶ãªè¡¨ç¾",
        "advice_title": "æ–‡åŒ–çš„ã‚¢ãƒ‰ãƒã‚¤ã‚¹",
        "voice_input_browser": "ğŸ¤ ãƒ–ãƒ©ã‚¦ã‚¶éŸ³å£°",
        # "voice_input_mic": "ğŸ™ï¸ ãƒã‚¤ã‚¯éŒ²éŸ³",  # æš‚æ—¶æ³¨é‡Š
    }
}


def play_text_js(text, lang):
    """
    Generate and execute JavaScript to play audio using the browser's built-in SpeechSynthesis API.
    This avoids backend network issues (403 Forbidden, connection errors) by running entirely in the client's browser.
    """
    lang_map = {
        "zh": "zh-CN",
        "en": "en-US",
        "ja": "ja-JP"
    }
    js_lang = lang_map.get(lang, "en-US")
    safe_text = json.dumps(text)
    
    js_code = f"""
    <script>
        try {{
            window.parent.speechSynthesis.cancel();
            var msg = new SpeechSynthesisUtterance({safe_text});
            msg.lang = "{js_lang}";
            msg.rate = 1.0; 
            window.parent.speechSynthesis.speak(msg);
        }} catch (e) {{
            console.error("TTS Error:", e);
        }}
    </script>
    """
    components.html(js_code, height=0)


def browser_speech_recognition_js(lang_code):
    """
    Simplified browser speech recognition with direct HTML rendering.
    å®Œå…¨ä¸ä¾èµ–ä»»ä½• APIï¼Œä½¿ç”¨æµè§ˆå™¨åŸç”ŸåŠŸèƒ½
    """
    lang_map = {"zh": "zh-CN", "en": "en-US", "ja": "ja-JP"}
    recognition_lang = lang_map.get(lang_code, "en-US")
    
    html_code = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <style>
            body {{
                font-family: Arial, sans-serif;
                padding: 20px;
                background: #f5f5f5;
            }}
            .container {{
                max-width: 600px;
                margin: 0 auto;
                background: white;
                padding: 20px;
                border-radius: 10px;
                box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            }}
            .status {{
                padding: 10px;
                margin: 10px 0;
                border-radius: 5px;
                text-align: center;
                font-weight: bold;
            }}
            .status.ready {{ background: #e3f2fd; color: #1976d2; }}
            .status.listening {{ background: #e8f5e9; color: #4caf50; }}
            .status.success {{ background: #c8e6c9; color: #2e7d32; }}
            .status.error {{ background: #ffcdd2; color: #c62828; }}
            
            #volumeBar {{
                width: 100%;
                height: 30px;
                background: #e0e0e0;
                border-radius: 15px;
                overflow: hidden;
                margin: 15px 0;
            }}
            #volumeLevel {{
                height: 100%;
                background: linear-gradient(90deg, #4caf50, #8bc34a);
                width: 0%;
                transition: width 0.1s ease;
            }}
            
            #resultBox {{
                min-height: 100px;
                padding: 15px;
                border: 2px solid #ddd;
                border-radius: 5px;
                margin: 15px 0;
                background: #fafafa;
                word-wrap: break-word;
                font-size: 16px;
            }}
            
            .interim {{ color: #999; font-style: italic; }}
            .final {{ color: #333; font-weight: bold; }}
            
            button {{
                padding: 12px 24px;
                margin: 5px;
                border: none;
                border-radius: 5px;
                font-size: 16px;
                cursor: pointer;
                transition: all 0.3s;
            }}
            button:hover {{ transform: translateY(-2px); }}
            .btn-start {{ background: #4caf50; color: white; }}
            .btn-stop {{ background: #f44336; color: white; }}
            .btn-copy {{ background: #2196f3; color: white; }}
            button:disabled {{ background: #ccc; cursor: not-allowed; }}
        </style>
    </head>
    <body>
        <div class="container">
            <div id="status" class="status ready">ğŸ¤ å‡†å¤‡å°±ç»ªï¼Œç‚¹å‡»å¼€å§‹æŒ‰é’®</div>
            
            <div id="volumeBar">
                <div id="volumeLevel"></div>
            </div>
            
            <div style="text-align: center; margin: 20px 0;">
                <button id="startBtn" class="btn-start" onclick="startRecognition()">ğŸ¤ å¼€å§‹è¯†åˆ«</button>
                <button id="stopBtn" class="btn-stop" onclick="stopRecognition()" disabled>â¹ï¸ åœæ­¢</button>
                <button id="copyBtn" class="btn-copy" onclick="copyResult()" disabled>ğŸ“‹ å¤åˆ¶ç»“æœ</button>
            </div>
            
            <div id="resultBox">ç­‰å¾…å¼€å§‹...</div>
        </div>

        <script>
            let recognition = null;
            let audioContext = null;
            let analyser = null;
            let microphone = null;
            let animationId = null;
            let finalTranscript = '';

            console.log('Script loaded');

            // æ£€æŸ¥æµè§ˆå™¨æ”¯æŒ
            if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {{
                document.getElementById('status').className = 'status error';
                document.getElementById('status').textContent = 'âŒ æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«';
                document.getElementById('resultBox').textContent = 'è¯·ä½¿ç”¨ Chrome æˆ– Edge æµè§ˆå™¨';
                document.getElementById('startBtn').disabled = true;
            }} else {{
                console.log('Speech recognition is supported');
            }}

            function updateStatus(message, className) {{
                const statusDiv = document.getElementById('status');
                statusDiv.textContent = message;
                statusDiv.className = 'status ' + className;
                console.log('Status:', message);
            }}

            function updateVolume() {{
                if (!analyser) return;
                
                const dataArray = new Uint8Array(analyser.frequencyBinCount);
                analyser.getByteFrequencyData(dataArray);
                
                const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
                const percentage = Math.min(100, (average / 128) * 100);
                
                document.getElementById('volumeLevel').style.width = percentage + '%';
                animationId = requestAnimationFrame(updateVolume);
            }}

            async function setupAudioMonitoring() {{
                console.log('Setting up audio monitoring...');
                try {{
                    const stream = await navigator.mediaDevices.getUserMedia({{ audio: true }});
                    console.log('Microphone access granted');
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    analyser = audioContext.createAnalyser();
                    microphone = audioContext.createMediaStreamSource(stream);
                    
                    analyser.fftSize = 256;
                    microphone.connect(analyser);
                    
                    updateVolume();
                    return stream;
                }} catch (err) {{
                    console.error('Microphone access error:', err);
                    updateStatus('âŒ æ— æ³•è®¿é—®éº¦å…‹é£ï¼š' + err.message, 'error');
                    throw err;
                }}
            }}

            async function startRecognition() {{
                console.log('Start button clicked');
                try {{
                    // è®¾ç½®éŸ³é‡ç›‘æ§
                    await setupAudioMonitoring();
                    
                    // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
                    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                    recognition = new SpeechRecognition();
                    console.log('SpeechRecognition initialized');
                    
                    recognition.lang = '{recognition_lang}';
                    recognition.continuous = true;
                    recognition.interimResults = true;
                    
                    finalTranscript = '';
                    
                    recognition.onstart = function() {{
                        console.log('Recognition started');
                        updateStatus('ğŸ¤ æ­£åœ¨ç›‘å¬... è¯·è¯´è¯', 'listening');
                        document.getElementById('startBtn').disabled = true;
                        document.getElementById('stopBtn').disabled = false;
                        document.getElementById('resultBox').innerHTML = '<span class="interim">ç­‰å¾…è¯­éŸ³è¾“å…¥...</span>';
                    }};
                    
                    recognition.onresult = function(event) {{
                        console.log('Recognition result received');
                        let interimTranscript = '';
                        
                        for (let i = event.resultIndex; i < event.results.length; i++) {{
                            const transcript = event.results[i][0].transcript;
                            if (event.results[i].isFinal) {{
                                finalTranscript += transcript + ' ';
                                console.log('Final:', transcript);
                            }} else {{
                                interimTranscript += transcript;
                                console.log('Interim:', transcript);
                            }}
                        }}
                        
                        let html = '';
                        if (finalTranscript) {{
                            html += '<span class="final">' + finalTranscript + '</span>';
                            updateStatus('âœ… è¯†åˆ«ä¸­... ç»§ç»­è¯´è¯æˆ–ç‚¹å‡»åœæ­¢', 'success');
                            document.getElementById('copyBtn').disabled = false;
                        }}
                        if (interimTranscript) {{
                            html += '<span class="interim">' + interimTranscript + '</span>';
                        }}
                        
                        document.getElementById('resultBox').innerHTML = html || '<span class="interim">ç­‰å¾…è¯­éŸ³è¾“å…¥...</span>';
                    }};
                    
                    recognition.onerror = function(event) {{
                        console.error('Recognition error:', event.error);
                        if (event.error === 'no-speech') {{
                            updateStatus('âš ï¸ æœªæ£€æµ‹åˆ°è¯­éŸ³', 'error');
                        }} else if (event.error === 'not-allowed') {{
                            updateStatus('âŒ éº¦å…‹é£æƒé™è¢«æ‹’ç»', 'error');
                        }} else {{
                            updateStatus('âŒ é”™è¯¯: ' + event.error, 'error');
                        }}
                    }};
                    
                    recognition.onend = function() {{
                        console.log('Recognition ended');
                        document.getElementById('startBtn').disabled = false;
                        document.getElementById('stopBtn').disabled = true;
                        
                        if (finalTranscript) {{
                            updateStatus('âœ… è¯†åˆ«å®Œæˆï¼ç‚¹å‡»å¤åˆ¶ç»“æœ', 'success');
                        }} else {{
                            updateStatus('â¸ï¸ å·²åœæ­¢', 'ready');
                        }}
                    }};
                    
                    console.log('Starting recognition...');
                    recognition.start();
                    
                }} catch (err) {{
                    console.error('Start error:', err);
                    updateStatus('âŒ å¯åŠ¨å¤±è´¥: ' + err.message, 'error');
                }}
            }}

            function stopRecognition() {{
                console.log('Stop button clicked');
                if (recognition) {{
                    recognition.stop();
                }}
                if (animationId) {{
                    cancelAnimationFrame(animationId);
                }}
                if (audioContext) {{
                    audioContext.close();
                }}
                document.getElementById('volumeLevel').style.width = '0%';
            }}

            function copyResult() {{
                console.log('Copy button clicked');
                const text = finalTranscript.trim();
                if (!text) {{
                    alert('æ²¡æœ‰å¯å¤åˆ¶çš„å†…å®¹');
                    return;
                }}
                
                navigator.clipboard.writeText(text).then(function() {{
                    console.log('Copied successfully');
                    const btn = document.getElementById('copyBtn');
                    btn.textContent = 'âœ… å·²å¤åˆ¶';
                    setTimeout(function() {{
                        btn.textContent = 'ğŸ“‹ å¤åˆ¶ç»“æœ';
                    }}, 2000);
                    
                    alert('è¯†åˆ«ç»“æœå·²å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼š\\n\\n' + text + '\\n\\nè¯·ç²˜è´´åˆ°è¾“å…¥æ¡†ä¸­ã€‚');
                }}.catch(function(err) {{
                    console.error('Copy failed:', err);
                    alert('å¤åˆ¶å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å¤åˆ¶ï¼š\\n\\n' + text);
                }});
            }}
        </script>
    </body>
    </html>
    """
    
    components.html(html_code, height=400, scrolling=False)


def recognize_speech_from_mic(lang_code):
    """
    Capture audio from the microphone and transcribe it.
    Prioritizes Google Speech Recognition (online).
    Falls back to Vosk (offline) if Google fails and Vosk model is available.
    """
    r = sr.Recognizer()
    
    # Map app language codes to Google Speech Recognition codes
    google_lang_map = {
        "zh": "zh-CN",
        "en": "en-US",
        "ja": "ja-JP"
    }
    target_lang = google_lang_map.get(lang_code, "en-US")

    # Map app language codes to Vosk model paths (relative to app.py)
    vosk_model_map = {
        "zh": "models/zh",
        "en": "models/en",
        "ja": "models/ja"
    }
    vosk_model_path = vosk_model_map.get(lang_code)

    try:
        with sr.Microphone() as source:
            st.info(f"ğŸ™ï¸ æ­£åœ¨ç›‘å¬ ({target_lang})...")
            r.adjust_for_ambient_noise(source, duration=0.5)
            audio = r.listen(source, timeout=5, phrase_time_limit=15)
            
            st.info("ğŸ”„ æ­£åœ¨è¯†åˆ«...")
            
            # 1. Try Google (Online)
            try:
                text = r.recognize_google(audio, language=target_lang)
                st.success("âœ… è¯†åˆ«æˆåŠŸï¼")
                return text
            except sr.RequestError as e:
                # Network error (e.g. GFW blocking Google)
                st.warning(f"âš ï¸ Google è¯­éŸ³æœåŠ¡è¿æ¥å¤±è´¥: {e}")
                
                # 2. Try Vosk (Offline) as fallback
                if vosk_model_path and os.path.exists(vosk_model_path):
                    st.info(f"ğŸ”„ å°è¯• Vosk ç¦»çº¿è¯†åˆ« (æ¨¡å‹: {vosk_model_path})...")
                    try:
                        from vosk import Model, KaldiRecognizer
                        
                        model = Model(vosk_model_path)
                        rec = KaldiRecognizer(model, 16000)
                        
                        # Convert audio data to bytes
                        data = audio.get_raw_data(convert_rate=16000, convert_width=2)
                        if rec.AcceptWaveform(data):
                            res = json.loads(rec.Result())
                            return res.get("text", "")
                        else:
                            res = json.loads(rec.FinalResult())
                            return res.get("text", "")
                            
                    except ImportError:
                        st.error("âŒ Vosk åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install vosk")
                    except Exception as vosk_e:
                        st.error(f"âŒ Vosk è¯†åˆ«å¤±è´¥: {vosk_e}")
                else:
                    st.error("âŒ ç¦»çº¿è¯†åˆ«ä¸å¯ç”¨ã€‚è¯·ä¸‹è½½ Vosk æ¨¡å‹å¹¶è§£å‹åˆ° `models/zh` (æˆ– en/ja) æ–‡ä»¶å¤¹ã€‚")
                    with st.expander("ğŸ“– å¦‚ä½•å¯ç”¨ç¦»çº¿è¯­éŸ³è¯†åˆ« (Vosk)"):
                        st.markdown("""
                        **æ­¥éª¤ï¼š**
                        1. ä¸‹è½½å¯¹åº”è¯­è¨€çš„æ¨¡å‹ (https://alphacephei.com/vosk/models)
                           - ä¸­æ–‡: `vosk-model-small-cn-0.22`
                           - è‹±æ–‡: `vosk-model-small-en-us-0.15`
                           - æ—¥æ–‡: `vosk-model-small-ja-0.22`
                        2. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `models` æ–‡ä»¶å¤¹
                        3. è§£å‹ä¸‹è½½çš„æ¨¡å‹ï¼Œé‡å‘½åä¸º `zh`ã€`en` æˆ– `ja`ï¼Œæ”¾å…¥ `models` æ–‡ä»¶å¤¹
                        4. å®‰è£… vosk: `pip install vosk`
                        """)
                return None
                
    except sr.WaitTimeoutError:
        st.warning("â±ï¸ æœªæ£€æµ‹åˆ°è¯­éŸ³")
    except sr.UnknownValueError:
        st.warning("â“ æ— æ³•ç†è§£éŸ³é¢‘å†…å®¹")
    except Exception as e:
        st.error(f"âŒ éº¦å…‹é£æˆ–ç³»ç»Ÿé”™è¯¯: {e}")
    return None


def main():
    # Sidebar for language selection
    with st.sidebar:
        st.header("Settings / è®¾ç½® / è¨­å®š")
        ui_lang_option = st.selectbox(
            "ç•Œé¢è¯­è¨€ / Interface Language",
            ["ä¸­æ–‡", "English", "æ—¥æœ¬èª"],
            index=0
        )
        
        # Show Python version
        import sys
        python_version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
        st.caption(f"ğŸ Python {python_version}")
        st.caption("âœ… éº¦å…‹é£å½•éŸ³ + æµè§ˆå™¨è¯­éŸ³è¾“å…¥")
    
    lang_code_map = {"ä¸­æ–‡": "zh", "English": "en", "æ—¥æœ¬èª": "ja"}
    ui_lang = lang_code_map[ui_lang_option]
    t = TRANSLATIONS[ui_lang]

    st.title(t["title"])
    st.write(t["version_info"])

    # Initialize session state for input text
    if "input_text" not in st.session_state:
        st.session_state.input_text = ""

    # 1. Language Selection
    col1, col2 = st.columns(2)
    with col1:
        source_lang = st.selectbox(t["source_lang"], ["zh", "en", "ja"], index=0,
                                   format_func=lambda x: {"zh": t["lang_zh"], "en": t["lang_en"], "ja": t["lang_ja"]}[x])
    with col2:
        target_lang = st.selectbox(t["target_lang"], ["zh", "en", "ja"], index=1,
                                   format_func=lambda x: {"zh": t["lang_zh"], "en": t["lang_en"], "ja": t["lang_ja"]}[x])

    # 2. Input Area with Voice Input Options
    st.subheader(t["input_label"])
    
    # Voice input buttons in columns
    voice_col1, voice_col2, voice_col3 = st.columns([1, 1, 3])
    
    with voice_col1:
        # Microphone Recording (traditional method)
        if st.button("ğŸ™ï¸ éº¦å…‹é£å½•éŸ³", key="mic_recording", use_container_width=True):
            recognized_text = recognize_speech_from_mic(source_lang)
            if recognized_text:
                st.session_state.input_text = recognized_text
                st.rerun()
    
    with voice_col2:
        # Browser Voice Input (Web Speech API)
        if st.button(t["voice_input_browser"], key="browser_voice", use_container_width=True):
            browser_speech_recognition_js(source_lang)
    
    with voice_col3:
        st.caption("ğŸ™ï¸ æ¨èä½¿ç”¨éº¦å…‹é£å½•éŸ³ | ğŸ¤ æµè§ˆå™¨è¯­éŸ³ä¸ºå¤‡é€‰æ–¹æ¡ˆ")
    
    # Text input area
    def update_input():
        st.session_state.input_text = st.session_state.widget_input

    source_text = st.text_area(
        label="Input Text",
        value=st.session_state.input_text,
        height=150,
        key="widget_input",
        on_change=update_input,
        label_visibility="collapsed",
        placeholder="åœ¨æ­¤è¾“å…¥æˆ–ç²˜è´´æ–‡æœ¬ï¼Œæˆ–ä½¿ç”¨ä¸Šæ–¹çš„è¯­éŸ³è¾“å…¥æŒ‰é’®..."
    )

    # 3. Other Parameters
    param_col1, param_col2 = st.columns(2)
    
    with param_col1:
        scenario = st.selectbox(
            t["scenario_label"],
            ["tourism", "dining", "casual_chat", "business"],
            index=0,
            format_func=lambda x: {
                "tourism": t["scenario_tourism"],
                "dining": t["scenario_dining"],
                "casual_chat": t["scenario_casual"],
                "business": t["scenario_business"],
            }[x],
        )
    
    with param_col2:
        tone = st.selectbox(
            t["tone_label"],
            ["casual", "neutral", "polite"],
            index=1,
            format_func=lambda x: {
                "casual": t["tone_casual"],
                "neutral": t["tone_neutral"],
                "polite": t["tone_polite"],
            }[x],
        )

    # 4. Translate Button
    if "translation_result" not in st.session_state:
        st.session_state.translation_result = None

    if st.button(t["translate_btn"], type="primary", use_container_width=True):
        if not source_text or not source_text.strip():
            st.warning(t["input_warning"])
        else:
            with st.spinner(t["spinner"]):
                result = generate_translation_and_advice(
                    source_text=source_text,
                    source_lang=source_lang,
                    target_lang=target_lang,
                    scenario=scenario,
                    tone=tone,
                )
                st.session_state.translation_result = result

    # 5. Results Display
    if st.session_state.translation_result:
        result = st.session_state.translation_result
        
        st.divider()
        
        # Literal Translation
        st.subheader(t["literal_title"])
        literal_text = result.get("literal_translation", "")
        st.write(literal_text)
        
        # TTS button for literal translation
        if literal_text and not literal_text.startswith("["):
            clean_literal = literal_text.replace("ç›´è¯‘ï¼š", "").strip()
            if st.button(t["tts_literal_btn"], key="tts_literal"):
                play_text_js(clean_literal, target_lang)

        st.divider()
        
        # Natural Expressions
        st.subheader(t["natural_title"])
        natural_data = result.get("natural_translation", [])
        
        if isinstance(natural_data, list):
            for idx, item in enumerate(natural_data):
                text = item.get("text", "")
                explanation = item.get("explanation", "")
                
                col_text, col_btn = st.columns([5, 1])
                with col_text:
                    st.markdown(f"**{idx + 1}. {text}**")
                    if explanation:
                        st.caption(explanation)
                with col_btn:
                    if text and not text.startswith("["):
                        if st.button("ğŸ”Š", key=f"tts_natural_{idx}"):
                            play_text_js(text, target_lang)
        else:
            st.write(natural_data)

        st.divider()
        
        # Cultural Advice
        st.subheader(t["advice_title"])
        st.markdown(result.get("advice", ""))


if __name__ == "__main__":
    main()
